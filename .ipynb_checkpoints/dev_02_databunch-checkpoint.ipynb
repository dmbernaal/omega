{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "This notebook extends basic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:04.496808Z",
     "iopub.status.busy": "2020-10-06T22:24:04.496808Z",
     "iopub.status.idle": "2020-10-06T22:24:06.727810Z",
     "shell.execute_reply": "2020-10-06T22:24:06.727810Z",
     "shell.execute_reply.started": "2020-10-06T22:24:04.496808Z"
    }
   },
   "outputs": [],
   "source": [
    "from nb.basic_data import stationary_close, scale_data, split_data, stack, create_gasfd_defaults, gaxf_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:07.279336Z",
     "iopub.status.busy": "2020-10-06T22:24:07.278338Z",
     "iopub.status.idle": "2020-10-06T22:24:07.772015Z",
     "shell.execute_reply": "2020-10-06T22:24:07.772015Z",
     "shell.execute_reply.started": "2020-10-06T22:24:07.279336Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as Basedataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:07.773013Z",
     "iopub.status.busy": "2020-10-06T22:24:07.773013Z",
     "iopub.status.idle": "2020-10-06T22:24:07.776006Z",
     "shell.execute_reply": "2020-10-06T22:24:07.776006Z",
     "shell.execute_reply.started": "2020-10-06T22:24:07.773013Z"
    }
   },
   "outputs": [],
   "source": [
    "from ta import add_all_ta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:07.778001Z",
     "iopub.status.busy": "2020-10-06T22:24:07.777003Z",
     "iopub.status.idle": "2020-10-06T22:24:07.785979Z",
     "shell.execute_reply": "2020-10-06T22:24:07.785979Z",
     "shell.execute_reply.started": "2020-10-06T22:24:07.778001Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading working data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:07.787974Z",
     "iopub.status.busy": "2020-10-06T22:24:07.787974Z",
     "iopub.status.idle": "2020-10-06T22:24:07.795952Z",
     "shell.execute_reply": "2020-10-06T22:24:07.795952Z",
     "shell.execute_reply.started": "2020-10-06T22:24:07.787974Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(root=None, remove=None, headers=None):\n",
    "    data = Path('./OmegaDev/Model_Z/raw_data/GBP_USD_H1_2016-01-01_2018-01-01.csv') if root is None else Path(root)\n",
    "    headers = ['date', 'complete', 'open', 'high', 'low', 'close', 'volume'] if headers is None else headers\n",
    "    df = pd.read_csv(data, header=None, names=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:07.920620Z",
     "iopub.status.busy": "2020-10-06T22:24:07.920620Z",
     "iopub.status.idle": "2020-10-06T22:24:07.956522Z",
     "shell.execute_reply": "2020-10-06T22:24:07.956522Z",
     "shell.execute_reply.started": "2020-10-06T22:24:07.920620Z"
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:08.322543Z",
     "iopub.status.busy": "2020-10-06T22:24:08.322543Z",
     "iopub.status.idle": "2020-10-06T22:24:08.328528Z",
     "shell.execute_reply": "2020-10-06T22:24:08.328528Z",
     "shell.execute_reply.started": "2020-10-06T22:24:08.322543Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_stack(ddd_GAXF, y_col=None, cols_to_ignore=None):\n",
    "    \"\"\"\n",
    "    Given ddd_GAXF where X is either s or d, we will form a new dataset which is 'stacked_features' where each feature in a single timestep is stacked -> similar to an image. \n",
    "    \n",
    "    The shape of each timestep will then become\n",
    "    \n",
    "    shape: (n, features, h, w)\n",
    "    \n",
    "    where\n",
    "        n: the number of timesteps\n",
    "        features: all the features to stack, this can differ with cols_to_ignore added\n",
    "        h: height\n",
    "        w: width\n",
    "        \n",
    "    This is very similar to an image with c=features channels \n",
    "    \"\"\"\n",
    "    c2i = [y_col] + [cols_to_ignore] if not isinstance(cols_to_ignore, list) else cols_to_ignore\n",
    "    cols2stack = [c for c in list(ddd_GAXF.keys()) if c not in c2i]\n",
    "    len_timesteps = len(ddd_GAXF[cols2stack[0]])\n",
    "    ddd_GAXF = {k:v[:,None,:,:] for k,v in ddd_GAXF.items() if k in cols2stack}\n",
    "    stacked_features = np.concatenate([v for _,v in ddd_GAXF.items() if _ in cols2stack], 1)\n",
    "    return stacked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:08.545978Z",
     "iopub.status.busy": "2020-10-06T22:24:08.545978Z",
     "iopub.status.idle": "2020-10-06T22:24:08.557917Z",
     "shell.execute_reply": "2020-10-06T22:24:08.556919Z",
     "shell.execute_reply.started": "2020-10-06T22:24:08.545978Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Basedataset):\n",
    "    \"\"\"\n",
    "    This is the core dataset class that is reponsible for iterating through our post-processed data. On init we will also take care of all pre-processing (scaling/etc)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, window, lbl_window, sl_pips=20, tp_pips=40, gaxf_type='gasf', ta_to_remove=None, im_size=None):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -----------\n",
    "            df: main dataframe class. For now this will only work with Oanda API data.\n",
    "            window: window size for the timeseries\n",
    "            lbl_window: window for prediction -> this preceds window\n",
    "            gaxf_type: the type of image transformation\n",
    "            im_size: size of the final image\n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        df = df.copy()\n",
    "        _remove = ['trend_psar_up', 'trend_psar_down']\n",
    "        if ta_to_remove != None:\n",
    "            ta_to_remove = [ta_to_remove] if not isinstance(ta_to_remove, list) else ta_to_remove\n",
    "            _remove = _remove + ta_to_remove\n",
    "        \n",
    "        ### ERROR CHECKING\n",
    "        if gaxf_type.lower() not in ['gasf', 'gadf']: raise ValueError('Wrong type')\n",
    "        gaxf_type=gaxf_type.lower()\n",
    "        \n",
    "        im_size = window if im_size is None else im_size\n",
    "        if im_size > window: raise ValueError('im_size cannot exceed window size')\n",
    "            \n",
    "        ### LABELING\n",
    "        ### Labeling here will be made via stationary_close. This can be turn into \n",
    "        ### a Regression or Classification task. We will follow through with Regression\n",
    "        df['close_label'] = df['close']\n",
    "        \n",
    "        ### TA FEATURES\n",
    "        ### For TA we will use all TA's and will exclude the ones we don't want\n",
    "        df = add_all_ta_features(df, 'open', 'high', 'low', 'close', 'volume')\n",
    "        df.drop(columns=_remove,axis=1,inplace=True)\n",
    "        \n",
    "        # dropna, the conversions won't work with NAN values\n",
    "        # TODO: fillna instead, find an appropriate method for this\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        ### DATA\n",
    "        ### This will only work with Oanda data for now.\n",
    "        data = df.iloc[:,2:].copy()\n",
    "        \n",
    "        ### SCALE -> using minmax\n",
    "        data_dd = scale_data(data, y_col='close_label')\n",
    "        \n",
    "        ### DATA SPLIT\n",
    "        ddd = split_data(data_dd, y_col='close_label', window_size=window, lbl_window=lbl_window, sl_pips=sl_pips, tp_pips=tp_pips)\n",
    "        \n",
    "        ### STACKING\n",
    "        ddd = stack(ddd)\n",
    "        \n",
    "        ### GASX CONVERSION\n",
    "        ddd = gaxf_convert(ddd, c2=gaxf_type, size=window)\n",
    "        \n",
    "        self.y = ddd['y']\n",
    "        self.data = feature_stack(ddd, y_col='y')\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx, :, :, :]\n",
    "        y = self.y[idx]\n",
    "        return torch.from_numpy(X).float(), torch.from_numpy(y).squeeze(0).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:09.133409Z",
     "iopub.status.busy": "2020-10-06T22:24:09.132379Z",
     "iopub.status.idle": "2020-10-06T22:24:09.140388Z",
     "shell.execute_reply": "2020-10-06T22:24:09.140388Z",
     "shell.execute_reply.started": "2020-10-06T22:24:09.133409Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataBunch:\n",
    "    \"\"\"\n",
    "    Wrapper class that creates sub Datasets for Train and Validation Splitting. \n",
    "    \"\"\"\n",
    "    def __init__(self, df, pct=0.2, window=24, lbl_window=4, gaxf_type='gasf', ta_to_remove=None, im_size=None):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "        --------------\n",
    "        ta_to_remove = ['others_dr', 'others_dlr', 'others_cr', 'momentum_rsi', 'momentum_tsi', 'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr', 'momentum_ao', 'momentum_kama', 'momentum_roc', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_psar_up_indicator', 'trend_psar_down_indicator', 'trend_aroon_down', 'trend_aroon_ind', 'trend_aroon_up']\n",
    "        \n",
    "        data = (DataBunch(df=df,\n",
    "                  pct=0.2,\n",
    "                  window=24,\n",
    "                  lbl_window=4,\n",
    "                  gaxf_type='gasf',\n",
    "                  ta_to_remove=ta_to_remove)\n",
    "       .bunch(bs=8))\n",
    "        \n",
    "        Params:\n",
    "        --------------\n",
    "            df: Main dataframe. This should be pre-data processing. Also known as the master dataframe\n",
    "            pct: percentage split for valid\n",
    "            window: window size\n",
    "            gaxf_type: gasf or gadf\n",
    "            ta_to_remove: technicals to not account for\n",
    "            im_size: size of gaxf image\n",
    "        \"\"\"\n",
    "        n = len(df)\n",
    "        valid_n = int(n * pct)\n",
    "        train_df = df.iloc[:-valid_n].copy()\n",
    "        valid_df = df.iloc[-valid_n:].copy()\n",
    "        \n",
    "        self.train_ds = Dataset(train_df, window=window, lbl_window=lbl_window, ta_to_remove=ta_to_remove, im_size=im_size)\n",
    "        self.valid_ds = Dataset(valid_df, window=window, lbl_window=lbl_window, ta_to_remove=ta_to_remove, im_size=im_size)\n",
    "        \n",
    "    def bunch(self, bs, num_workers=0, shuffle=False):\n",
    "        self.train_dl = DataLoader(self.train_ds, batch_size=bs, num_workers=num_workers, shuffle=shuffle)\n",
    "        self.valid_dl = DataLoader(self.valid_ds, batch_size=bs, num_workers=num_workers, shuffle=False)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:09.410635Z",
     "iopub.status.busy": "2020-10-06T22:24:09.410635Z",
     "iopub.status.idle": "2020-10-06T22:24:09.414624Z",
     "shell.execute_reply": "2020-10-06T22:24:09.414624Z",
     "shell.execute_reply.started": "2020-10-06T22:24:09.410635Z"
    }
   },
   "outputs": [],
   "source": [
    "ta_to_remove = ['others_dr', 'others_dlr', 'others_cr', 'momentum_rsi', 'momentum_tsi', 'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr', 'momentum_ao', 'momentum_kama', 'momentum_roc', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_psar_up_indicator', 'trend_psar_down_indicator', 'trend_aroon_down', 'trend_aroon_ind', 'trend_aroon_up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:24:09.648000Z",
     "iopub.status.busy": "2020-10-06T22:24:09.648000Z",
     "iopub.status.idle": "2020-10-06T22:25:28.061624Z",
     "shell.execute_reply": "2020-10-06T22:25:28.061624Z",
     "shell.execute_reply.started": "2020-10-06T22:24:09.648000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmber\\Anaconda3\\lib\\site-packages\\ta\\trend.py:608: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i]/self._trs[i])\n",
      "C:\\Users\\dmber\\Anaconda3\\lib\\site-packages\\ta\\trend.py:612: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i]/self._trs[i])\n",
      "C:\\Users\\dmber\\Anaconda3\\lib\\site-packages\\ta\\trend.py:608: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i]/self._trs[i])\n",
      "C:\\Users\\dmber\\Anaconda3\\lib\\site-packages\\ta\\trend.py:612: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i]/self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "data = (DataBunch(df=df,\n",
    "                  pct=0.2,\n",
    "                  window=24,\n",
    "                  lbl_window=4,\n",
    "                  gaxf_type='gasf',\n",
    "                  ta_to_remove=ta_to_remove)\n",
    "       .bunch(bs=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
