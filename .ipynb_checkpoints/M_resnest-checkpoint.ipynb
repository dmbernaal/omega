{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T01:26:31.069863Z",
     "iopub.status.busy": "2020-10-04T01:26:31.069863Z",
     "iopub.status.idle": "2020-10-04T01:26:31.074873Z",
     "shell.execute_reply": "2020-10-04T01:26:31.074873Z",
     "shell.execute_reply.started": "2020-10-04T01:26:31.069863Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from nb.activations import Swish, Mila, Mish, BentID\n",
    "from torch.nn.init import zeros_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet variant\n",
    "This variant is specific for ResNeSt architecture. We will initially write up a ```Bottleneck``` approach, however, will include a ```Basicblock``` in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T00:26:40.210468Z",
     "iopub.status.busy": "2020-10-04T00:26:40.209444Z",
     "iopub.status.idle": "2020-10-04T00:26:40.214427Z",
     "shell.execute_reply": "2020-10-04T00:26:40.214427Z",
     "shell.execute_reply.started": "2020-10-04T00:26:40.210468Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_activation_(act='relu', inplace=True):\n",
    "    _activations_ = nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=inplace)],\n",
    "        ['lrelu', nn.LeakyReLU(inplace=inplace)],\n",
    "        ['swish', Swish()],\n",
    "        ['bent_id', BentID()],\n",
    "        ['mila', Mila()],\n",
    "        ['mish', Mish()]\n",
    "    ])\n",
    "    return _activations_[act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T00:45:43.990740Z",
     "iopub.status.busy": "2020-10-04T00:45:43.990740Z",
     "iopub.status.idle": "2020-10-04T00:45:43.996724Z",
     "shell.execute_reply": "2020-10-04T00:45:43.996724Z",
     "shell.execute_reply.started": "2020-10-04T00:45:43.990740Z"
    }
   },
   "outputs": [],
   "source": [
    "class rSoftMax(nn.Module):\n",
    "    def __init__(self, radix, cardinality):\n",
    "        super(rSoftMax, self).__init__()\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch = x.size(0)\n",
    "        if self.radix > 1:\n",
    "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = x.reshape(batch, -1)\n",
    "        else: x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T03:57:40.756496Z",
     "iopub.status.busy": "2020-10-04T03:57:40.756496Z",
     "iopub.status.idle": "2020-10-04T03:57:40.772013Z",
     "shell.execute_reply": "2020-10-04T03:57:40.771492Z",
     "shell.execute_reply.started": "2020-10-04T03:57:40.756496Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split Attention Conv2d\n",
    "\"\"\"\n",
    "class SplAtConv2d(nn.Module):\n",
    "    def __init__(self, ni, nf, ks, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1, bias=True, radix=2, reduction_factor=4, norm_layer=None, act='relu', **kwargs):\n",
    "        super(SplAtConv2d, self).__init__()\n",
    "        padding = _pair(padding)\n",
    "        inter_channels = max(ni*radix//reduction_factor, 32)\n",
    "        self.use_bn = norm_layer is not None\n",
    "        self.radix = radix\n",
    "        self.cardinality = groups\n",
    "        self.channels = nf\n",
    "        if self.use_bn:\n",
    "            self.bn0 = nn.BatchNorm2d(nf*radix)\n",
    "            self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "            bias = False\n",
    "        self.conv = nn.Conv2d(ni, nf*radix, kernel_size=ks, stride=stride, padding=padding, dilation=dilation, groups=groups*radix, bias=bias, **kwargs)\n",
    "        self.act_fn = get_activation_(act)\n",
    "        self.conv_fc1 = nn.Conv2d(nf, inter_channels, 1, groups=self.cardinality)\n",
    "        self.conv_fc2 = nn.Conv2d(inter_channels, nf*radix, 1, groups=self.cardinality)\n",
    "        self.rsoftmax = rSoftMax(radix, groups)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_bn: x = self.bn0(x)\n",
    "        x = self.act_fn(x)\n",
    "        \n",
    "        batch, rchannel = x.shape[:2]\n",
    "        \n",
    "        if self.radix > 1:\n",
    "            splitted = torch.split(x, rchannel//self.radix, dim=1)\n",
    "            gap = sum(splitted)\n",
    "        else: gap = x\n",
    "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
    "        gap = self.conv_fc1(gap)\n",
    "        \n",
    "        if self.use_bn: gap = self.bn1(gap)\n",
    "        gap = self.act_fn(gap)\n",
    "        \n",
    "        atten = self.conv_fc2(gap)\n",
    "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
    "        \n",
    "        if self.radix > 1:\n",
    "            attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
    "            out = sum([attn*split for (attn,split) in zip(attens, splitted)])\n",
    "        else: out = atten * x\n",
    "        \n",
    "        return out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T03:57:41.153726Z",
     "iopub.status.busy": "2020-10-04T03:57:41.153726Z",
     "iopub.status.idle": "2020-10-04T03:57:41.158692Z",
     "shell.execute_reply": "2020-10-04T03:57:41.158692Z",
     "shell.execute_reply.started": "2020-10-04T03:57:41.153726Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
    "    def __init__(self): super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x): return F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T03:57:41.412105Z",
     "iopub.status.busy": "2020-10-04T03:57:41.411129Z",
     "iopub.status.idle": "2020-10-04T03:57:41.415652Z",
     "shell.execute_reply": "2020-10-04T03:57:41.415100Z",
     "shell.execute_reply.started": "2020-10-04T03:57:41.412105Z"
    }
   },
   "outputs": [],
   "source": [
    "class Noop(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T03:57:41.619860Z",
     "iopub.status.busy": "2020-10-04T03:57:41.619860Z",
     "iopub.status.idle": "2020-10-04T03:57:41.637811Z",
     "shell.execute_reply": "2020-10-04T03:57:41.637811Z",
     "shell.execute_reply.started": "2020-10-04T03:57:41.619860Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, radix=1, cardinality=1, bottleneck_width=64, avd=False, avd_first=False, dilation=1, is_first=False, norm_layer=None, last_gamma=False, act='relu'):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        group_width = int(planes * (bottleneck_width/64.)) * cardinality\n",
    "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        self.radix = radix\n",
    "        self.avd = avd and (stride > 1 or is_first)\n",
    "        self.avd_first = avd_first\n",
    "        \n",
    "        if self.avd:\n",
    "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
    "            stride = 1\n",
    "            \n",
    "        if radix >= 1:\n",
    "            self.conv2 = SplAtConv2d(\n",
    "                group_width, group_width, ks=3,\n",
    "                stride=stride, padding=dilation,\n",
    "                dilation=dilation, groups=cardinality, bias=False,\n",
    "                radix=radix, norm_layer=norm_layer, act=act)\n",
    "        else:\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                group_width, group_width, kernel_size=3,\n",
    "                stride=stride, padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(group_width)\n",
    "            \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            group_width, planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
    "        \n",
    "        if last_gamma: zeros_(self.bn3.weight)\n",
    "        \n",
    "        self.act = get_activation_(act)\n",
    "        self.downsample = downsample\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act(out)\n",
    "        if self.avd and self.avd_first: out = self.avd_layer(out)\n",
    "            \n",
    "        out = self.conv2(out)\n",
    "        if self.radix==0:\n",
    "            out = self.bn2(out)\n",
    "            out = self.act(out)\n",
    "        if self.avd and not self.avd_first: out = self.avd_layer(out)\n",
    "            \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None: residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:26.403810Z",
     "iopub.status.busy": "2020-10-04T04:06:26.402812Z",
     "iopub.status.idle": "2020-10-04T04:06:26.430768Z",
     "shell.execute_reply": "2020-10-04T04:06:26.430768Z",
     "shell.execute_reply.started": "2020-10-04T04:06:26.403810Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64, c_in=3, num_classes=1000, dilated=False, dilation=1, deep_stem=False, stem_width=64, avg_down=False, avd=False, avd_first=False, final_drop=0.0, last_gamma=False, norm_layer=True, act='relu'):\n",
    "        self.cardinality = groups\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        self.inplanes = stem_width*2 if deep_stem else 64\n",
    "        self.avg_down = avg_down\n",
    "        self.last_gamma = last_gamma\n",
    "        self.norm_layer = norm_layer\n",
    "        self.radix = radix\n",
    "        self.avd = avd\n",
    "        self.avd_first = avd_first\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        self.act_fn = get_activation_(act)\n",
    "        conv_layer = nn.Conv2d\n",
    "        conv_kwargs = {}\n",
    "        if deep_stem:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                conv_layer(c_in, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
    "                nn.BatchNorm2d(stem_width) if norm_layer else Noop(), act_fn,\n",
    "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
    "                nn.BatchNorm2d(stem_width) if norm_layer else Noop(), act_fn,\n",
    "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs))\n",
    "        else: self.conv1 = conv_layer(c_in, 64, kernel_size=7, stride=3, padding=3, bias=False, **conv_kwargs)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes) if norm_layer else None\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False, act=act)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer, act=act)\n",
    "        \n",
    "        if dilated or dilation==4:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer, act=act)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer, act=act)\n",
    "        elif dilation==2:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilation=1, norm_layer=norm_layer, act=act)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2, norm_layer=norm_layer, act=act)\n",
    "        else:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer, act=act)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer, act=act)\n",
    "        \n",
    "        self.avgpool = GlobalAvgPool2d()\n",
    "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. /n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None, is_first=True, act='relu'):\n",
    "        downsample = None\n",
    "        \n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            down_layers = []\n",
    "            if self.avg_down:\n",
    "                if dilation == 1: down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False))\n",
    "                else: down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1, ceil_mode=True, count_include_pad=False))\n",
    "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=1, bias=False))\n",
    "            else: down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False))\n",
    "            if self.norm_layer: down_layers.append(nn.BatchNorm2d(planes*block.expansion))\n",
    "            downsample = nn.Sequential(*down_layers)\n",
    "        \n",
    "        layers = []\n",
    "        if dilation == 1 or dilation == 2:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality, bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first, dilation=1, is_first=is_first, norm_layer=norm_layer,\n",
    "                                last_gamma=self.last_gamma, act=act))\n",
    "        elif dilation==4:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality, bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first, dilation=2, is_first=is_first, norm_layer=norm_layer,\n",
    "                                last_gamma=self.last_gamma, act=act))\n",
    "        else: raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
    "        \n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width, avd=self.avd,\n",
    "                                avd_first=self.avd_first, dilation=dilation, norm_layer=norm_layer,\n",
    "                                last_gamma=self.last_gamma, act=act))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.bn1: x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.drop: x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:26.639179Z",
     "iopub.status.busy": "2020-10-04T04:06:26.639179Z",
     "iopub.status.idle": "2020-10-04T04:06:26.644166Z",
     "shell.execute_reply": "2020-10-04T04:06:26.644166Z",
     "shell.execute_reply.started": "2020-10-04T04:06:26.639179Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnest50(c_in=3, num_classes=1000, act='relu', **kwargs):\n",
    "    layers = [3, 4, 5, 6]\n",
    "    model = ResNet(Bottleneck, layers,\n",
    "                   radix=2, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=False, c_in=c_in, num_classes=num_classes, act=act, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:26.803763Z",
     "iopub.status.busy": "2020-10-04T04:06:26.803763Z",
     "iopub.status.idle": "2020-10-04T04:06:26.808726Z",
     "shell.execute_reply": "2020-10-04T04:06:26.808726Z",
     "shell.execute_reply.started": "2020-10-04T04:06:26.803763Z"
    }
   },
   "outputs": [],
   "source": [
    "def mininest(c_in=3, num_classes=1000, act='relu', **kwargs):\n",
    "    layers = [1, 1, 1, 1]\n",
    "    model = ResNet(Bottleneck, layers,\n",
    "                   radix=2, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=False, c_in=c_in, num_classes=num_classes, act=act, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:41.763989Z",
     "iopub.status.busy": "2020-10-04T04:06:41.763989Z",
     "iopub.status.idle": "2020-10-04T04:06:42.241795Z",
     "shell.execute_reply": "2020-10-04T04:06:42.241795Z",
     "shell.execute_reply.started": "2020-10-04T04:06:41.763989Z"
    }
   },
   "outputs": [],
   "source": [
    "m1 = resnest50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:42.242713Z",
     "iopub.status.busy": "2020-10-04T04:06:42.242713Z",
     "iopub.status.idle": "2020-10-04T04:06:42.380382Z",
     "shell.execute_reply": "2020-10-04T04:06:42.380382Z",
     "shell.execute_reply.started": "2020-10-04T04:06:42.242713Z"
    }
   },
   "outputs": [],
   "source": [
    "mn = mininest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:42.706473Z",
     "iopub.status.busy": "2020-10-04T04:06:42.706473Z",
     "iopub.status.idle": "2020-10-04T04:06:42.709465Z",
     "shell.execute_reply": "2020-10-04T04:06:42.709465Z",
     "shell.execute_reply.started": "2020-10-04T04:06:42.706473Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T04:06:55.898341Z",
     "iopub.status.busy": "2020-10-04T04:06:55.897320Z",
     "iopub.status.idle": "2020-10-04T04:06:56.045922Z",
     "shell.execute_reply": "2020-10-04T04:06:56.045922Z",
     "shell.execute_reply.started": "2020-10-04T04:06:55.898341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             864\n",
      "       BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "              ReLU-3           [-1, 32, 64, 64]               0\n",
      "            Conv2d-4           [-1, 32, 64, 64]           9,216\n",
      "       BatchNorm2d-5           [-1, 32, 64, 64]              64\n",
      "              ReLU-6           [-1, 32, 64, 64]               0\n",
      "            Conv2d-7           [-1, 64, 64, 64]          18,432\n",
      "       BatchNorm2d-8           [-1, 64, 64, 64]             128\n",
      "              ReLU-9           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-10           [-1, 64, 32, 32]               0\n",
      "           Conv2d-11           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
      "             ReLU-13           [-1, 64, 32, 32]               0\n",
      "           Conv2d-14          [-1, 128, 32, 32]          36,864\n",
      "      BatchNorm2d-15          [-1, 128, 32, 32]             256\n",
      "             ReLU-16          [-1, 128, 32, 32]               0\n",
      "           Conv2d-17             [-1, 32, 1, 1]           2,080\n",
      "      BatchNorm2d-18             [-1, 32, 1, 1]              64\n",
      "             ReLU-19             [-1, 32, 1, 1]               0\n",
      "           Conv2d-20            [-1, 128, 1, 1]           4,224\n",
      "         rSoftMax-21                  [-1, 128]               0\n",
      "      SplAtConv2d-22           [-1, 64, 32, 32]               0\n",
      "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
      "        AvgPool2d-25           [-1, 64, 32, 32]               0\n",
      "           Conv2d-26          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-27          [-1, 256, 32, 32]             512\n",
      "             ReLU-28          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-29          [-1, 256, 32, 32]               0\n",
      "           Conv2d-30          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-31          [-1, 128, 32, 32]             256\n",
      "             ReLU-32          [-1, 128, 32, 32]               0\n",
      "           Conv2d-33          [-1, 256, 32, 32]         147,456\n",
      "      BatchNorm2d-34          [-1, 256, 32, 32]             512\n",
      "             ReLU-35          [-1, 256, 32, 32]               0\n",
      "           Conv2d-36             [-1, 64, 1, 1]           8,256\n",
      "      BatchNorm2d-37             [-1, 64, 1, 1]             128\n",
      "             ReLU-38             [-1, 64, 1, 1]               0\n",
      "           Conv2d-39            [-1, 256, 1, 1]          16,640\n",
      "         rSoftMax-40                  [-1, 256]               0\n",
      "      SplAtConv2d-41          [-1, 128, 32, 32]               0\n",
      "        AvgPool2d-42          [-1, 128, 16, 16]               0\n",
      "           Conv2d-43          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "        AvgPool2d-45          [-1, 256, 16, 16]               0\n",
      "           Conv2d-46          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-48          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-49          [-1, 512, 16, 16]               0\n",
      "           Conv2d-50          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-51          [-1, 256, 16, 16]             512\n",
      "             ReLU-52          [-1, 256, 16, 16]               0\n",
      "           Conv2d-53          [-1, 512, 16, 16]         589,824\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-55          [-1, 512, 16, 16]               0\n",
      "           Conv2d-56            [-1, 128, 1, 1]          32,896\n",
      "      BatchNorm2d-57            [-1, 128, 1, 1]             256\n",
      "             ReLU-58            [-1, 128, 1, 1]               0\n",
      "           Conv2d-59            [-1, 512, 1, 1]          66,048\n",
      "         rSoftMax-60                  [-1, 512]               0\n",
      "      SplAtConv2d-61          [-1, 256, 16, 16]               0\n",
      "        AvgPool2d-62            [-1, 256, 8, 8]               0\n",
      "           Conv2d-63           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-64           [-1, 1024, 8, 8]           2,048\n",
      "        AvgPool2d-65            [-1, 512, 8, 8]               0\n",
      "           Conv2d-66           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-67           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-68           [-1, 1024, 8, 8]               0\n",
      "       Bottleneck-69           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-70            [-1, 512, 8, 8]         524,288\n",
      "      BatchNorm2d-71            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-72            [-1, 512, 8, 8]               0\n",
      "           Conv2d-73           [-1, 1024, 8, 8]       2,359,296\n",
      "      BatchNorm2d-74           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-75           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-76            [-1, 256, 1, 1]         131,328\n",
      "      BatchNorm2d-77            [-1, 256, 1, 1]             512\n",
      "             ReLU-78            [-1, 256, 1, 1]               0\n",
      "           Conv2d-79           [-1, 1024, 1, 1]         263,168\n",
      "         rSoftMax-80                 [-1, 1024]               0\n",
      "      SplAtConv2d-81            [-1, 512, 8, 8]               0\n",
      "        AvgPool2d-82            [-1, 512, 4, 4]               0\n",
      "           Conv2d-83           [-1, 2048, 4, 4]       1,048,576\n",
      "      BatchNorm2d-84           [-1, 2048, 4, 4]           4,096\n",
      "        AvgPool2d-85           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-86           [-1, 2048, 4, 4]       2,097,152\n",
      "      BatchNorm2d-87           [-1, 2048, 4, 4]           4,096\n",
      "             ReLU-88           [-1, 2048, 4, 4]               0\n",
      "       Bottleneck-89           [-1, 2048, 4, 4]               0\n",
      "  GlobalAvgPool2d-90                 [-1, 2048]               0\n",
      "           Linear-91                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 10,611,688\n",
      "Trainable params: 10,611,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 59.38\n",
      "Params size (MB): 40.48\n",
      "Estimated Total Size (MB): 100.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(mn, (3, 128, 128), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
