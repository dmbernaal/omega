{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T13:59:28.673807Z",
     "iopub.status.busy": "2020-10-04T13:59:28.672810Z",
     "iopub.status.idle": "2020-10-04T13:59:29.685668Z",
     "shell.execute_reply": "2020-10-04T13:59:29.685668Z",
     "shell.execute_reply.started": "2020-10-04T13:59:28.673807Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from nb.activations import Swish, Mila, Mish, BentID\n",
    "from torch.nn.init import zeros_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet variant\n",
    "This variant is specific for ResNeSt architecture. We will initially write up a ```Bottleneck``` approach, however, will include a ```Basicblock``` in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T13:59:30.191259Z",
     "iopub.status.busy": "2020-10-04T13:59:30.191259Z",
     "iopub.status.idle": "2020-10-04T13:59:30.196246Z",
     "shell.execute_reply": "2020-10-04T13:59:30.196246Z",
     "shell.execute_reply.started": "2020-10-04T13:59:30.191259Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_activation_(act='relu', inplace=True):\n",
    "    _activations_ = nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=inplace)],\n",
    "        ['lrelu', nn.LeakyReLU(inplace=inplace)],\n",
    "        ['swish', Swish()],\n",
    "        ['bent_id', BentID()],\n",
    "        ['mila', Mila()],\n",
    "        ['mish', Mish()]\n",
    "    ])\n",
    "    return _activations_[act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T13:59:30.802649Z",
     "iopub.status.busy": "2020-10-04T13:59:30.801628Z",
     "iopub.status.idle": "2020-10-04T13:59:30.807610Z",
     "shell.execute_reply": "2020-10-04T13:59:30.807610Z",
     "shell.execute_reply.started": "2020-10-04T13:59:30.802649Z"
    }
   },
   "outputs": [],
   "source": [
    "class rSoftMax(nn.Module):\n",
    "    def __init__(self, radix, cardinality):\n",
    "        super(rSoftMax, self).__init__()\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch = x.size(0)\n",
    "        if self.radix > 1:\n",
    "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = x.reshape(batch, -1)\n",
    "        else: x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T13:59:31.192816Z",
     "iopub.status.busy": "2020-10-04T13:59:31.191818Z",
     "iopub.status.idle": "2020-10-04T13:59:31.204782Z",
     "shell.execute_reply": "2020-10-04T13:59:31.204782Z",
     "shell.execute_reply.started": "2020-10-04T13:59:31.192816Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split Attention Conv2d\n",
    "\"\"\"\n",
    "class SplAtConv2d(nn.Module):\n",
    "    def __init__(self, ni, nf, ks, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1, bias=True, radix=2, reduction_factor=4, norm_layer=None, act='relu', **kwargs):\n",
    "        super(SplAtConv2d, self).__init__()\n",
    "        padding = _pair(padding)\n",
    "        inter_channels = max(ni*radix//reduction_factor, 32)\n",
    "        self.use_bn = norm_layer is not None\n",
    "        self.radix = radix\n",
    "        self.cardinality = groups\n",
    "        self.channels = nf\n",
    "        if self.use_bn:\n",
    "            self.bn0 = nn.BatchNorm2d(nf*radix)\n",
    "            self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "            bias = False\n",
    "        self.conv = nn.Conv2d(ni, nf*radix, kernel_size=ks, stride=stride, padding=padding, dilation=dilation, groups=groups*radix, bias=bias, **kwargs)\n",
    "        self.act_fn = get_activation_(act)\n",
    "        self.conv_fc1 = nn.Conv2d(nf, inter_channels, 1, groups=self.cardinality)\n",
    "        self.conv_fc2 = nn.Conv2d(inter_channels, nf*radix, 1, groups=self.cardinality)\n",
    "        self.rsoftmax = rSoftMax(radix, groups)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_bn: x = self.bn0(x)\n",
    "        x = self.act_fn(x)\n",
    "        \n",
    "        batch, rchannel = x.shape[:2]\n",
    "        \n",
    "        if self.radix > 1:\n",
    "            splitted = torch.split(x, rchannel//self.radix, dim=1)\n",
    "            gap = sum(splitted)\n",
    "        else: gap = x\n",
    "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
    "        gap = self.conv_fc1(gap)\n",
    "        \n",
    "        if self.use_bn: gap = self.bn1(gap)\n",
    "        gap = self.act_fn(gap)\n",
    "        \n",
    "        atten = self.conv_fc2(gap)\n",
    "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
    "        \n",
    "        if self.radix > 1:\n",
    "            attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
    "            out = sum([attn*split for (attn,split) in zip(attens, splitted)])\n",
    "        else: out = atten * x\n",
    "        \n",
    "        return out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T13:59:31.649593Z",
     "iopub.status.busy": "2020-10-04T13:59:31.648595Z",
     "iopub.status.idle": "2020-10-04T13:59:31.653583Z",
     "shell.execute_reply": "2020-10-04T13:59:31.653583Z",
     "shell.execute_reply.started": "2020-10-04T13:59:31.649593Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
    "    def __init__(self): super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x): return F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T13:59:32.113276Z",
     "iopub.status.busy": "2020-10-04T13:59:32.112280Z",
     "iopub.status.idle": "2020-10-04T13:59:32.117265Z",
     "shell.execute_reply": "2020-10-04T13:59:32.117265Z",
     "shell.execute_reply.started": "2020-10-04T13:59:32.113276Z"
    }
   },
   "outputs": [],
   "source": [
    "class Noop(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:24:13.411813Z",
     "iopub.status.busy": "2020-10-04T14:24:13.411813Z",
     "iopub.status.idle": "2020-10-04T14:24:13.427804Z",
     "shell.execute_reply": "2020-10-04T14:24:13.427804Z",
     "shell.execute_reply.started": "2020-10-04T14:24:13.411813Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, radix=1, cardinality=1, bottleneck_width=64, avd=False, avd_first=False, dilation=1, is_first=False, norm_layer=None, last_gamma=False, act='relu'):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        group_width = int(planes * (bottleneck_width/64.)) * cardinality\n",
    "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        self.radix = radix\n",
    "        self.avd = avd and (stride > 1 or is_first)\n",
    "        self.avd_first = avd_first\n",
    "        \n",
    "        if self.avd:\n",
    "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
    "            stride = 1\n",
    "            \n",
    "        if radix >= 1:\n",
    "            self.conv2 = SplAtConv2d(\n",
    "                group_width, group_width, ks=3,\n",
    "                stride=stride, padding=dilation,\n",
    "                dilation=dilation, groups=cardinality, bias=False,\n",
    "                radix=radix, norm_layer=norm_layer, act=act)\n",
    "        else:\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                group_width, group_width, kernel_size=3,\n",
    "                stride=stride, padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(group_width)\n",
    "            \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            group_width, planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
    "        \n",
    "        if last_gamma: zeros_(self.bn3.weight)\n",
    "        \n",
    "        self.act = get_activation_(act)\n",
    "        self.downsample = downsample\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act(out)\n",
    "        if self.avd and self.avd_first: out = self.avd_layer(out)\n",
    "            \n",
    "        out = self.conv2(out)\n",
    "        if self.radix==0:\n",
    "            out = self.bn2(out)\n",
    "            out = self.act(out)\n",
    "        if self.avd and not self.avd_first: out = self.avd_layer(out)\n",
    "            \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None: residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:32.214872Z",
     "iopub.status.busy": "2020-10-04T14:26:32.214872Z",
     "iopub.status.idle": "2020-10-04T14:26:32.225883Z",
     "shell.execute_reply": "2020-10-04T14:26:32.225883Z",
     "shell.execute_reply.started": "2020-10-04T14:26:32.214872Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, radix=1, cardinality=1, bottleneck_width=64, avd=False, avd_first=False, dilation=1, is_first=False, norm_layer=None, last_gamma=False, act='relu'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        group_width = int(planes * (bottleneck_width/64.)) * cardinality\n",
    "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=3, bias=False, stride=stride, padding=dilation, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        self.radix = radix\n",
    "        self.avd = avd and (stride > 1 or is_first)\n",
    "        if self.avd: stride = 1\n",
    "            \n",
    "        if radix >= 1:\n",
    "            self.conv2 = SplAtConv2d(\n",
    "                group_width, group_width, ks=3,\n",
    "                stride=stride, padding=dilation,\n",
    "                dilation=dilation, groups=cardinality, bias=False,\n",
    "                radix=radix, norm_layer=norm_layer, act=act)\n",
    "        else:\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                group_width, group_width, kernel_size=3,\n",
    "                stride=stride, padding=dilation, dilation=dilation,\n",
    "                groups=cardinality, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        \n",
    "        if last_gamma: zeros_(self.bn2.weight)\n",
    "        \n",
    "        self.act = get_activation_(act)\n",
    "        self.downsample = downsample\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act(out)\n",
    "            \n",
    "        out = self.conv2(out)\n",
    "        if self.radix==0:\n",
    "            out = self.bn2(out)\n",
    "            \n",
    "        if self.downsample is not None: residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:32.694589Z",
     "iopub.status.busy": "2020-10-04T14:26:32.694589Z",
     "iopub.status.idle": "2020-10-04T14:26:32.778398Z",
     "shell.execute_reply": "2020-10-04T14:26:32.778398Z",
     "shell.execute_reply.started": "2020-10-04T14:26:32.694589Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64, c_in=3, num_classes=1000, dilated=False, dilation=1, deep_stem=False, stem_width=64, avg_down=False, avd=False, avd_first=False, final_drop=0.0, last_gamma=False, norm_layer=True, act='relu'):\n",
    "        self.cardinality = groups\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        self.inplanes = stem_width*2 if deep_stem else 64\n",
    "        self.avg_down = avg_down\n",
    "        self.last_gamma = last_gamma\n",
    "        self.norm_layer = norm_layer\n",
    "        self.radix = radix\n",
    "        self.avd = avd\n",
    "        self.avd_first = avd_first\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        self.act_fn = get_activation_(act)\n",
    "        conv_layer = nn.Conv2d\n",
    "        conv_kwargs = {}\n",
    "        if deep_stem:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                conv_layer(c_in, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
    "                nn.BatchNorm2d(stem_width) if norm_layer else Noop(), self.act_fn,\n",
    "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
    "                nn.BatchNorm2d(stem_width) if norm_layer else Noop(), self.act_fn,\n",
    "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs))\n",
    "        else: self.conv1 = conv_layer(c_in, 64, kernel_size=7, stride=3, padding=3, bias=False, **conv_kwargs)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes) if norm_layer else None\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False, act=act)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer, act=act)\n",
    "        \n",
    "        if dilated or dilation==4:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer, act=act)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer, act=act)\n",
    "        elif dilation==2:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilation=1, norm_layer=norm_layer, act=act)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2, norm_layer=norm_layer, act=act)\n",
    "        else:\n",
    "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer, act=act)\n",
    "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer, act=act)\n",
    "        \n",
    "        self.avgpool = GlobalAvgPool2d()\n",
    "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. /n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None, is_first=True, act='relu'):\n",
    "        downsample = None\n",
    "        \n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            down_layers = []\n",
    "            if self.avg_down:\n",
    "                if dilation == 1: down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False))\n",
    "                else: down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1, ceil_mode=True, count_include_pad=False))\n",
    "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=1, bias=False))\n",
    "            else: down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False))\n",
    "            if self.norm_layer: down_layers.append(nn.BatchNorm2d(planes*block.expansion))\n",
    "            downsample = nn.Sequential(*down_layers)\n",
    "        \n",
    "        layers = []\n",
    "        if dilation == 1 or dilation == 2:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality, bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first, dilation=1, is_first=is_first, norm_layer=norm_layer,\n",
    "                                last_gamma=self.last_gamma, act=act))\n",
    "        elif dilation==4:\n",
    "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
    "                                radix=self.radix, cardinality=self.cardinality, bottleneck_width=self.bottleneck_width,\n",
    "                                avd=self.avd, avd_first=self.avd_first, dilation=2, is_first=is_first, norm_layer=norm_layer,\n",
    "                                last_gamma=self.last_gamma, act=act))\n",
    "        else: raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
    "        \n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes,\n",
    "                                radix=self.radix, cardinality=self.cardinality,\n",
    "                                bottleneck_width=self.bottleneck_width, avd=self.avd,\n",
    "                                avd_first=self.avd_first, dilation=dilation, norm_layer=norm_layer,\n",
    "                                last_gamma=self.last_gamma, act=act))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.bn1: x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if self.drop: x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:32.870119Z",
     "iopub.status.busy": "2020-10-04T14:26:32.870119Z",
     "iopub.status.idle": "2020-10-04T14:26:32.876107Z",
     "shell.execute_reply": "2020-10-04T14:26:32.875139Z",
     "shell.execute_reply.started": "2020-10-04T14:26:32.870119Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnest50(c_in=3, num_classes=1000, act='relu', **kwargs):\n",
    "    layers = [3, 4, 5, 6]\n",
    "    model = ResNet(Bottleneck, layers,\n",
    "                   radix=2, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=False, c_in=c_in, num_classes=num_classes, act=act, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:33.036674Z",
     "iopub.status.busy": "2020-10-04T14:26:33.036674Z",
     "iopub.status.idle": "2020-10-04T14:26:33.042659Z",
     "shell.execute_reply": "2020-10-04T14:26:33.042659Z",
     "shell.execute_reply.started": "2020-10-04T14:26:33.036674Z"
    }
   },
   "outputs": [],
   "source": [
    "def mininest_ba(c_in=3, num_classes=1000, act='relu', **kwargs):\n",
    "    layers = [1, 1, 1, 1]\n",
    "    model = ResNet(BasicBlock, layers,\n",
    "                   radix=2, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=False, c_in=c_in, num_classes=num_classes, act=act, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:33.199239Z",
     "iopub.status.busy": "2020-10-04T14:26:33.199239Z",
     "iopub.status.idle": "2020-10-04T14:26:33.203261Z",
     "shell.execute_reply": "2020-10-04T14:26:33.203261Z",
     "shell.execute_reply.started": "2020-10-04T14:26:33.199239Z"
    }
   },
   "outputs": [],
   "source": [
    "def mininest_bn(c_in=3, num_classes=1000, act='relu', **kwargs):\n",
    "    layers = [1, 1, 1, 1]\n",
    "    model = ResNet(Bottleneck, layers,\n",
    "                   radix=2, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=False, c_in=c_in, num_classes=num_classes, act=act, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:33.351833Z",
     "iopub.status.busy": "2020-10-04T14:26:33.351833Z",
     "iopub.status.idle": "2020-10-04T14:26:33.879636Z",
     "shell.execute_reply": "2020-10-04T14:26:33.879636Z",
     "shell.execute_reply.started": "2020-10-04T14:26:33.351833Z"
    }
   },
   "outputs": [],
   "source": [
    "m1 = resnest50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:33.881630Z",
     "iopub.status.busy": "2020-10-04T14:26:33.881630Z",
     "iopub.status.idle": "2020-10-04T14:26:34.091102Z",
     "shell.execute_reply": "2020-10-04T14:26:34.091102Z",
     "shell.execute_reply.started": "2020-10-04T14:26:33.881630Z"
    }
   },
   "outputs": [],
   "source": [
    "mn_ba = mininest_ba()\n",
    "mn_bn = mininest_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:26:34.092067Z",
     "iopub.status.busy": "2020-10-04T14:26:34.092067Z",
     "iopub.status.idle": "2020-10-04T14:26:34.096056Z",
     "shell.execute_reply": "2020-10-04T14:26:34.095097Z",
     "shell.execute_reply.started": "2020-10-04T14:26:34.092067Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-04T14:27:06.290004Z",
     "iopub.status.busy": "2020-10-04T14:27:06.290004Z",
     "iopub.status.idle": "2020-10-04T14:27:06.374777Z",
     "shell.execute_reply": "2020-10-04T14:27:06.374777Z",
     "shell.execute_reply.started": "2020-10-04T14:27:06.290004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             864\n",
      "       BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "              ReLU-3           [-1, 32, 64, 64]               0\n",
      "              ReLU-4           [-1, 32, 64, 64]               0\n",
      "            Conv2d-5           [-1, 32, 64, 64]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 64, 64]              64\n",
      "              ReLU-7           [-1, 32, 64, 64]               0\n",
      "              ReLU-8           [-1, 32, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          18,432\n",
      "      BatchNorm2d-10           [-1, 64, 64, 64]             128\n",
      "             ReLU-11           [-1, 64, 64, 64]               0\n",
      "             ReLU-12           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-13           [-1, 64, 32, 32]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "             ReLU-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17          [-1, 128, 32, 32]          36,864\n",
      "      BatchNorm2d-18          [-1, 128, 32, 32]             256\n",
      "             ReLU-19          [-1, 128, 32, 32]               0\n",
      "           Conv2d-20             [-1, 32, 1, 1]           2,080\n",
      "      BatchNorm2d-21             [-1, 32, 1, 1]              64\n",
      "             ReLU-22             [-1, 32, 1, 1]               0\n",
      "           Conv2d-23            [-1, 128, 1, 1]           4,224\n",
      "         rSoftMax-24                  [-1, 128]               0\n",
      "      SplAtConv2d-25           [-1, 64, 32, 32]               0\n",
      "             ReLU-26           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "             ReLU-30          [-1, 128, 16, 16]               0\n",
      "           Conv2d-31          [-1, 256, 16, 16]         147,456\n",
      "      BatchNorm2d-32          [-1, 256, 16, 16]             512\n",
      "             ReLU-33          [-1, 256, 16, 16]               0\n",
      "           Conv2d-34             [-1, 64, 1, 1]           8,256\n",
      "      BatchNorm2d-35             [-1, 64, 1, 1]             128\n",
      "             ReLU-36             [-1, 64, 1, 1]               0\n",
      "           Conv2d-37            [-1, 256, 1, 1]          16,640\n",
      "         rSoftMax-38                  [-1, 256]               0\n",
      "      SplAtConv2d-39          [-1, 128, 16, 16]               0\n",
      "        AvgPool2d-40           [-1, 64, 16, 16]               0\n",
      "           Conv2d-41          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
      "             ReLU-43          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-44          [-1, 128, 16, 16]               0\n",
      "           Conv2d-45            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-46            [-1, 256, 8, 8]             512\n",
      "             ReLU-47            [-1, 256, 8, 8]               0\n",
      "           Conv2d-48            [-1, 512, 8, 8]         589,824\n",
      "      BatchNorm2d-49            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-50            [-1, 512, 8, 8]               0\n",
      "           Conv2d-51            [-1, 128, 1, 1]          32,896\n",
      "      BatchNorm2d-52            [-1, 128, 1, 1]             256\n",
      "             ReLU-53            [-1, 128, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]          66,048\n",
      "         rSoftMax-55                  [-1, 512]               0\n",
      "      SplAtConv2d-56            [-1, 256, 8, 8]               0\n",
      "        AvgPool2d-57            [-1, 128, 8, 8]               0\n",
      "           Conv2d-58            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
      "             ReLU-60            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-61            [-1, 256, 8, 8]               0\n",
      "           Conv2d-62            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-63            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-64            [-1, 512, 4, 4]               0\n",
      "           Conv2d-65           [-1, 1024, 4, 4]       2,359,296\n",
      "      BatchNorm2d-66           [-1, 1024, 4, 4]           2,048\n",
      "             ReLU-67           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-68            [-1, 256, 1, 1]         131,328\n",
      "      BatchNorm2d-69            [-1, 256, 1, 1]             512\n",
      "             ReLU-70            [-1, 256, 1, 1]               0\n",
      "           Conv2d-71           [-1, 1024, 1, 1]         263,168\n",
      "         rSoftMax-72                 [-1, 1024]               0\n",
      "      SplAtConv2d-73            [-1, 512, 4, 4]               0\n",
      "        AvgPool2d-74            [-1, 256, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-78            [-1, 512, 4, 4]               0\n",
      "  GlobalAvgPool2d-79                  [-1, 512]               0\n",
      "           Linear-80                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 5,965,544\n",
      "Trainable params: 5,965,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 28.90\n",
      "Params size (MB): 22.76\n",
      "Estimated Total Size (MB): 51.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(mn_ba, (3, 128, 128), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
